Noi vogliamo che il computer  sia in grado di differenziare tutte le imagini, quindi in grado di 
identificare alcune feature che possono esserci in alcune immagini e non in altre.
Per esempio noi possiamo classificare una immagine di  un cane se essa ha 4 zampe, im modo 
similare un computer potrebbe essere in grado di classificare immagini cercando feature a 
basso livello come archi e curve.

Reti Convoluzionali e Deep Learning

L'idea nasce da come funziona  la corteccia visiva.
Una CNN prende l'immagine la passa attraverso una serie di filtri --> convolutiona, pooling e 
fully connected layers.

Quindi il primo livello è sempre un Convolutional Layer, infatti l'immagine viene convoluta con 
un filtro (chiamato neurone o kernel) il quale appunto viene, appunto fatto scorrere sull'immagine,
il filtro è una matrice di numeri chiamati pesi o parametri, e mentre il filtroo scorre attorno all'immagine di input,
moltiplica i valori nel filtro con i pixel originali nell'immagine, e poi vengono sommate.
Viene poi spostato il filtro di un valore chiamato stride --> ogni posizione produce un numero , 
e otteniamo in output una serie di numeri di dimensionalità minore o uguale rispetto alla
immagine originale --> e quello che otteniamo viene chiamato MAPPA DI ATTIVAZIONE O MAPPA 
DELLE CARATTERISRICHE (feature map).

Ognuno di questi filtri è un IDENTIFICATORE DI FEATURE, infatti la moltiplicazione e la somma dei valori èu un numero alto se la feature viene identificata.

*****************************
"cosa in più"
vengono cambiati solo i pesi deli filtri --> se una filtro riconsoce una linea in una regione spaziale la riconoscerà in qualsiasi altra regione spaziale
******************************

O = (W-F+2P)/S +1
Esempio di filtri

Una rete neurale convoluzionale a il seguente schema : (Immagine)


ReLU Layers (o activation layer)--> lo scopo di questo layer è introdurre la non linearità nel sistema (che prima era lineare convolutional operation).
Nel passato coe funzioni non lineari erano usate funzioni coe tanh o sigmmoidi --> ma qua i ricercatori hanno trovato ReLU, che funziona meglio perchè la rete si addestra più velocemente senza perdere troppo di accuratezza, viene applicata la funzione f(x) = max(0,x), cambia solo i valori negativi in 0.


Dopo il ReLU layers, i programmatori possono scegliere di applicare il polling layer, il filtro piùbase è che prende un filtro di dimensione 2x2 e un stride (passo) della stessa lunghezza, e l'output è il massimo numero di ogni sottoregione che il filtro convoluisce.
Riduce di molto lo spazio, infatti il numeor dei parametri o pesi viene ridotto e cosi la computazione, il seconod è che permette di controllare l'overfitting.


FC--> layer finale per ottenere solo neuroni (quindi valori scalari) --> il che porta all'uso 
delle reti neurali classiche