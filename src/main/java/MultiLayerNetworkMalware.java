import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.distribution.Distribution;
import org.deeplearning4j.nn.conf.inputs.InputType;
import org.deeplearning4j.nn.conf.layers.ConvolutionLayer;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.conf.layers.SubsamplingLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.learning.config.Nesterovs;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.nd4j.linalg.lossfunctions.impl.LossMCXENT;

public class MultiLayerNetworkMalware {

    private int channels;
    private int width;
    private int height;
    private long seed;
    private double dropout;
    private double learningRate;
    private int numLabels;
    private double l2;
    private int iterations;

    public MultiLayerNetworkMalware(int channels, int width, int height, long seed, double dropout, double learningRate, int numLabels, double l2, int iterations) {
        this.channels = channels;
        this.width = width;
        this.height = height;
        this.seed = seed;
        this.dropout = dropout;
        this.learningRate = learningRate;
        this.numLabels = numLabels;
        this.l2 = l2;
        this.iterations = iterations;
    }

    public  MultiLayerNetwork getVGG16(){

        ConvolutionLayer.AlgoMode cudnnAlgoMode = ConvolutionLayer.AlgoMode.NO_WORKSPACE;

        MultiLayerConfiguration conf =
                new NeuralNetConfiguration.Builder()
                .learningRate(learningRate)
                .regularization(true).l2(l2)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .updater(Updater.NESTEROVS).activation(Activation.RELU)
                .momentum(0.9)
                .weightInit(WeightInit.XAVIER)
                .list()
                    // block 1
                    .layer(0, new ConvolutionLayer.Builder().kernelSize(3,3).stride(1,1)
                                    .padding(1,1).nIn(channels).nOut(64)
                                    .cudnnAlgoMode(ConvolutionLayer.AlgoMode.NO_WORKSPACE).build())
                    .layer(1, new ConvolutionLayer.Builder().kernelSize(3,3).stride(1,1)
                                    .padding(1,1).nOut(64)
                                    .cudnnAlgoMode(ConvolutionLayer.AlgoMode.NO_WORKSPACE).build())
                    .layer(2, new SubsamplingLayer.Builder()
                            .poolingType(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2)
                            .stride(2, 2).build())

                    // block 2
                    .layer(3, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(128).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(4, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(128).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(5, new SubsamplingLayer.Builder()
                            .poolingType(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2)
                            .stride(2, 2).build())

                    // block 3
                    .layer(6, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(256).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(7, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(256).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(8, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(256).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(9, new SubsamplingLayer.Builder()
                            .poolingType(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2)
                            .stride(2, 2).build())

                    // block 4
                    .layer(10, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(512).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(11, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(512).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(12, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(512).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(13, new SubsamplingLayer.Builder()
                            .poolingType(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2)
                            .stride(2, 2).build())

                    // block 5
                    .layer(14, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(512).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(15, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(512).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(16, new ConvolutionLayer.Builder().kernelSize(3, 3).stride(1, 1)
                            .padding(1, 1).nOut(512).cudnnAlgoMode(cudnnAlgoMode).build())
                    .layer(17, new SubsamplingLayer.Builder()
                            .poolingType(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2)
                            .stride(2, 2).build())
                    .layer(18, new OutputLayer.Builder(
                            LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).name("output")
                            .nOut(numLabels).activation(Activation.SOFTMAX) // radial basis function required
                            .build())
                    .backprop(true).pretrain(false).setInputType(InputType
                    .convolutionalFlat(height, width, channels))
                    .build();

        return new MultiLayerNetwork(conf);
    }

    public MultiLayerNetwork malwareNet() {

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .iterations(iterations)
                .regularization(true).l2(l2) // tried 0.0001, 0.0005
                .activation(Activation.RELU)
                .learningRate(learningRate)// tried 0.00001, 0.00005, 0.000001
                .weightInit(WeightInit.XAVIER_LEGACY)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .updater(new Nesterovs(0.9))
                .list()
                .layer(0, convInit("cnn1", channels, 50, new int[]{5, 5}, new int[]{1, 1}, new int[]{0, 0}, 0))
                .layer(1, new SubsamplingLayer.Builder(new int[]{2,2}, new int[]{1, 1}).name("maxpool").build())
                .layer(2, conv5x5("cnn2", 100, new int[]{5, 5}, new int[]{1, 1}, 0))
                .layer(3, new SubsamplingLayer.Builder(new int[]{2,2}, new int[]{1, 1}).name("maxpool2").build())
                .layer(4, new DenseLayer.Builder().nOut(1024).build())
                .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                        .nOut(numLabels)
                        .activation(Activation.SOFTMAX)
                        .build())
                .backprop(true).pretrain(false)
                .setInputType(InputType.convolutional(height, width, channels))
                .build();

        return new MultiLayerNetwork(conf);
    }
    public MultiLayerNetwork malwareNetWeighted(INDArray weightArray) {
        /*
            if the use case is dealing with class imbalance for classification, use smaller weights for frequently occurring
            classes, and 1.0 or larger weights for infrequently occurring classes.
        */
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .iterations(iterations)
                .regularization(true).l2(l2)
                .activation(Activation.RELU)
                .learningRate(learningRate)
                .weightInit(WeightInit.XAVIER_LEGACY)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .updater(new Nesterovs(0.9))
                .list()
                .layer(0, convInit("cnn1", channels, 50, new int[]{5, 5}, new int[]{1, 1}, new int[]{0, 0}, 0))
                .layer(1, new SubsamplingLayer.Builder(new int[]{2,2}, new int[]{1, 1}).name("maxpool").build())
                .layer(2, conv5x5("cnn2", 100, new int[]{5, 5}, new int[]{1, 1}, 0))
                .layer(3, new SubsamplingLayer.Builder(new int[]{2,2}, new int[]{1, 1}).name("maxpool2").build())
                .layer(4, new DenseLayer.Builder().nOut(1024).build())
                .layer(5, new OutputLayer.Builder()
                        .lossFunction(new LossMCXENT(weightArray))
                        .nOut(numLabels)
                        .activation(Activation.SOFTMAX)
                        .build())
                .backprop(true).pretrain(false)
                .setInputType(InputType.convolutional(height, width, channels))
                .build();

        return new MultiLayerNetwork(conf);
    }

    // my lenet model
    public MultiLayerNetwork leNetModel() {
        // best conf now lr = 0.1, l2 = 0.01, nout = 50,100,500 , 200 batchsize, 50 epoche
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .iterations(iterations)
                .regularization(true).l2(l2)
                .activation(Activation.RELU)
                .learningRate(learningRate)
                .weightInit(WeightInit.XAVIER_LEGACY)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .updater(new Nesterovs(0.9))
                .list()//change out original are 50,100,200
                .layer(0, convInit("cnn1", channels, 100, new int[]{5, 5}, new int[]{1, 1}, new int[]{0, 0}, 0))
                .layer(1, maxPool("maxpool1", new int[]{2, 2}))
                .layer(2, conv5x5("cnn2", 200, new int[]{5, 5}, new int[]{1, 1}, 0))
                .layer(3, maxPool("maxpool2", new int[]{2, 2}))
                .layer(4, new DenseLayer.Builder().nOut(500).build())
                .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                        .nOut(numLabels)
                        .activation(Activation.SOFTMAX)
                        .build())
                .backprop(true).pretrain(false)
                .setInputType(InputType.convolutional(height, width, channels))
                .build();

        return new MultiLayerNetwork(conf);
    }

    private ConvolutionLayer convInit(String name, int in, int out, int[] kernel, int[] stride, int[] pad, double bias) {
        return new ConvolutionLayer.Builder(kernel, stride, pad).name(name).nIn(in).nOut(out).biasInit(bias).build();
    }

    private ConvolutionLayer conv3x3(String name, int out, double bias) {
        return new ConvolutionLayer.Builder(new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1}).name(name).nOut(out).biasInit(bias).build();
    }

    private ConvolutionLayer conv5x5(String name, int out, int[] stride, int[] pad, double bias) {
        return new ConvolutionLayer.Builder(new int[]{5, 5}, stride, pad).name(name).nOut(out).biasInit(bias).build();
    }

    private SubsamplingLayer maxPool(String name, int[] kernel) {
        return new SubsamplingLayer.Builder(kernel, new int[]{2, 2}).name(name).build();
    }

    private DenseLayer fullyConnected(String name, int out, double bias, double dropOut, Distribution dist) {
        return new DenseLayer.Builder().name(name).nOut(out).biasInit(bias).dropOut(dropOut).dist(dist).build();
    }

}
